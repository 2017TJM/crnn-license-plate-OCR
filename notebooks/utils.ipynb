{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class strLabelConverter(object):\n",
    "    \"\"\"Convert between str and label.\n",
    "    NOTE:\n",
    "        Insert `blank` to the alphabet for CTC.\n",
    "    Args:\n",
    "        alphabet (str): set of the possible characters.\n",
    "        ignore_case (bool, default=True): whether or not to ignore all of the case.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alphabet, ignore_case=True):\n",
    "        self._ignore_case = ignore_case\n",
    "        if self._ignore_case:\n",
    "            alphabet = alphabet.lower()\n",
    "        self.alphabet = alphabet + '-'  # for `-1` index\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(alphabet):\n",
    "            # NOTE: 0 is reserved for 'blank' required by wrap_ctc\n",
    "            self.dict[char] = i + 1\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"Support batch or single str.\n",
    "        Args:\n",
    "            text (str or list of str): texts to convert.\n",
    "        Returns:\n",
    "            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n",
    "            torch.IntTensor [n]: length of each text.\n",
    "        \"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [\n",
    "                self.dict[char.lower() if self._ignore_case else char]\n",
    "                for char in text\n",
    "            ]\n",
    "            length = [len(text)]\n",
    "        elif isinstance(text, collections.Iterable):\n",
    "            length = [len(s) for s in text]\n",
    "            text = ''.join(text)\n",
    "            text, _ = self.encode(text)\n",
    "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
    "\n",
    "    def decode(self, t, length, raw=False):\n",
    "        \"\"\"Decode encoded texts back into strs.\n",
    "        Args:\n",
    "            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n",
    "            torch.IntTensor [n]: length of each text.\n",
    "        Raises:\n",
    "            AssertionError: when the texts and its length does not match.\n",
    "        Returns:\n",
    "            text (str or list of str): texts to convert.\n",
    "        \"\"\"\n",
    "        if length.numel() == 1:\n",
    "            length = length[0]\n",
    "            assert t.numel() == length, \"text with length: {} does not match declared length: {}\".format(t.numel(), length)\n",
    "            if raw:\n",
    "                return ''.join([self.alphabet[i - 1] for i in t])\n",
    "            else:\n",
    "                char_list = []\n",
    "                for i in range(length):\n",
    "                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n",
    "                        char_list.append(self.alphabet[t[i] - 1])\n",
    "                return ''.join(char_list)\n",
    "        else:\n",
    "            # batch mode\n",
    "            assert t.numel() == length.sum(), \"texts with length: {} does not match declared length: {}\".format(t.numel(), length.sum())\n",
    "            texts = []\n",
    "            index = 0\n",
    "            for i in range(length.numel()):\n",
    "                l = length[i]\n",
    "                texts.append(\n",
    "                    self.decode(\n",
    "                        t[index:index + l], torch.IntTensor([l]), raw=raw))\n",
    "                index += l\n",
    "            return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class averager(object):\n",
    "    \"\"\"Compute average for `torch.Variable` and `torch.Tensor`. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def add(self, v):\n",
    "        if isinstance(v, Variable):\n",
    "            count = v.data.numel()\n",
    "            v = v.data.sum()\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            count = v.numel()\n",
    "            v = v.sum()\n",
    "\n",
    "        self.n_count += count\n",
    "        self.sum += v\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_count = 0\n",
    "        self.sum = 0\n",
    "\n",
    "    def val(self):\n",
    "        res = 0\n",
    "        if self.n_count != 0:\n",
    "            res = self.sum / float(self.n_count)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(v, v_length, nc):\n",
    "    batchSize = v_length.size(0)\n",
    "    maxLength = v_length.max()\n",
    "    v_onehot = torch.FloatTensor(batchSize, maxLength, nc).fill_(0)\n",
    "    acc = 0\n",
    "    for i in range(batchSize):\n",
    "        length = v_length[i]\n",
    "        label = v[acc:acc + length].view(-1, 1).long()\n",
    "        v_onehot[i, :length].scatter_(1, label, 1.0)\n",
    "        acc += length\n",
    "    return v_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(v, data):\n",
    "    v.resize_(data.size()).copy_(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyPrint(v):\n",
    "    print('Size {0}, Type: {1}'.format(str(v.size()), v.data.type()))\n",
    "    print('| Max: %f | Min: %f | Mean: %f' % (v.max().data[0], v.min().data[0],\n",
    "                                              v.mean().data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assureRatio(img):\n",
    "    \"\"\"Ensure imgH <= imgW.\"\"\"\n",
    "    b, c, h, w = img.size()\n",
    "    if h > w:\n",
    "        main = nn.UpsamplingBilinear2d(size=(h, h), scale_factor=None)\n",
    "        img = main(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_idx(mat):\n",
    "    \"\"\"\n",
    "    Return the coordinate of the largest value in an 2D-array.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : a list of tuples (coordinate)\n",
    "    \"\"\"\n",
    "    indices = np.where(mat==mat.max())\n",
    "    return list(zip(indices[0], indices[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dup_argmax_idx(mat):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the indices of columns of duplicated prediction pairs (after removing \"empty\" prediction)\n",
    "    NOTE: duplicated \"empty\" prediction pairs are ignored\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : a list containing all indices of duplicated prediction pairs columns \n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "    \"\"\"\n",
    "    dup_col_idx = []\n",
    "\n",
    "    first_occurance_idx = 0\n",
    "    first_occurance_argmax = 0 \n",
    "\n",
    "    for col_idx, argmax in enumerate(mat.argmax(0)):\n",
    "        \n",
    "        # we only consider if argmax is not 0 (\"empty\")\n",
    "        if argmax != 0:\n",
    "            # if the current argmax is same as immediate last argmax \n",
    "            if (argmax == first_occurance_argmax):\n",
    "                # record down these duplicated columns index\n",
    "                dup_col_idx = dup_col_idx + [first_occurance_idx] + [col_idx]\n",
    "\n",
    "                # reset the first occurance index and value (this will ensure next loop goes to \"else\", \n",
    "                # i.e: setting next \"non-empty\" column as \"first_occurance\" \n",
    "                \n",
    "                first_occurance_idx = 0\n",
    "                first_occurance_argmax = 0 \n",
    "\n",
    "            # current non-zero argmax is different from immediate last argmax\n",
    "            else:\n",
    "                # set this as new first occurance index and value\n",
    "                first_occurance_idx = col_idx\n",
    "                first_occurance_argmax = argmax\n",
    "\n",
    "    return(dup_col_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore')\n",
    "\n",
    "def pred_conf(out_mat, num_top_results):\n",
    "    \"\"\"\n",
    "    Compute the top predicted sequences and their corresponding prediction confidence from the output matrix\n",
    "    of neural network.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    out_mat: output matrix from crnn\n",
    "    num_top_results: number of top prediction results\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Assumption:\n",
    "    1) Initial \"empty\" predictions are accurate\n",
    "    2) Duplicated predictions (e.g \"--W--W\" > \"W\") are accurate \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out: a list of tuples containing (predicted sequence, prediction's confidence)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    out_mat = out_mat.copy() # to avoid changing the original output matrix\n",
    "    seq = [] # predicted sequence (before decoding to alphanumeric)\n",
    "    seq_act = [] # corresponding activations of the predicted sequence\n",
    "    logits_sum = np.exp(out_mat).sum(axis=0, dtype='f') # sum(exp(score))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # top predicted sequence\n",
    "    top_seq = list(out_mat.argmax(0))\n",
    "    seq.append(top_seq)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PRELIMINARY FILTERING (based on the assumptions)\n",
    "    # Criteria 1: ignore \"empty\" prediction\n",
    "    # Criterai 2: ignore \"duplicate prediction\"\n",
    "    empty_col_idx = np.where(out_mat.argmax(0)==0)[0].tolist()\n",
    "    dup_col_idx = dup_argmax_idx(out_mat)\n",
    "    \n",
    "    # set these columns to -INF\n",
    "    out_mat[:, empty_col_idx + dup_col_idx] = np.NINF\n",
    "    \n",
    "    \n",
    "    \n",
    "    top_seq_act = []\n",
    "    # Captures the activations of top predicted sequence and set it to -INF afterwards\n",
    "    for col, argmax in enumerate(top_seq):\n",
    "        if np.isfinite(out_mat[argmax, col]):\n",
    "            top_seq_act.append(out_mat[argmax, col])\n",
    "            out_mat[argmax, col] = np.NINF\n",
    "        else:\n",
    "            top_seq_act.append(np.nan)\n",
    "    \n",
    "    seq_act.append(top_seq_act)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # For second top predictions onward\n",
    "    for top_n_result in range(1, num_top_results):\n",
    "        # if the entire out_mat become -inf, there isn't more possible predictions\n",
    "        if np.all(~np.isfinite(out_mat)):\n",
    "            print(f\"Only {top_n_result} possible results\")\n",
    "            break\n",
    "        \n",
    "        # find the coordinates of the next most probable predictions in output matrix (after \"PRELIMINARY FILTERING\")\n",
    "        # we're interested to know the position of the current \"highest\" activation in the output matrix\n",
    "        coord = max_idx(out_mat)\n",
    "        for (argmax, col) in coord:\n",
    "            \n",
    "            # the next most probable sequence is the same as immediate last sequence, EXCEPT for 1 character, \n",
    "            # which is the character in the position of max_idx\n",
    "            next_seq = seq[-1].copy()\n",
    "            next_seq_act = seq_act[-1].copy()\n",
    "            \n",
    "            # change whatever character in the position to character indicated by max_idx\n",
    "            next_seq[col] = argmax\n",
    "            next_seq_act[col] = out_mat[argmax, col]\n",
    "            \n",
    "            seq.append(next_seq)\n",
    "            seq_act.append(next_seq_act)\n",
    "\n",
    "            # reset the current highest activation to -NINF \n",
    "            out_mat[(argmax, col)] = np.NINF\n",
    "            \n",
    "            \n",
    "            \n",
    "    # calculate logits from the activations (logits = exp(act))\n",
    "    logits = np.exp(seq_act)\n",
    "    # individual prediction probability (prob = logit / sum(logits) in the same column) (softmax)\n",
    "    prob_ind = np.divide(logits, logits_sum)\n",
    "    \n",
    "    ## EXTREME CASES CONSIDERATION\n",
    "    \n",
    "    # 1) prob_ind contains infinity values\n",
    "    # - happens when sum(logits) = 0\n",
    "    prob_ind[np.isinf(prob_ind)] = np.nan\n",
    "    \n",
    "    # 2) entire prob_ind matrix is np.nan\n",
    "    # - happens when input image is irrelevant \n",
    "    # - many \"empty\" => np.nan && \"non-empty columns\" has sum(logits) == 0 (Extreme case 1)\n",
    "    if np.all(np.isnan(prob_ind)):\n",
    "        prob_total = np.zeros(num_top_results)\n",
    "    else:\n",
    "        prob_total = np.nanprod(prob_ind, axis=1)\n",
    "        \n",
    "        # 3) final probability is more than 1 \n",
    "        # - rounding error could happen\n",
    "        prob_total[prob_total>1] = 1\n",
    "    \n",
    "    \n",
    "    return list(zip(np.asarray(seq), prob_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctcBestPath(mat, classes):\n",
    "    \"implements best path decoding as shown by Graves (Dissertation, p63)\"\n",
    "\n",
    "    # dim0=t, dim1=c\n",
    "    maxT, maxC = mat.shape\n",
    "    label = ''\n",
    "    blankIdx = len(classes)\n",
    "    lastMaxIdx = maxC # init with invalid label\n",
    "\n",
    "    for t in range(maxT):\n",
    "        maxIdx = np.argmax(mat[t, :])\n",
    "\n",
    "        if maxIdx != lastMaxIdx and maxIdx != blankIdx:\n",
    "            label += classes[maxIdx]\n",
    "\n",
    "        lastMaxIdx = maxIdx\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamEntry:\n",
    "    \"information about one single beam at specific time-step\"\n",
    "    def __init__(self):\n",
    "        self.prTotal = 0 # blank and non-blank\n",
    "        self.prNonBlank = 0 # non-blank\n",
    "        self.prBlank = 0 # blank\n",
    "        self.prText = 1 # LM score\n",
    "        self.lmApplied = False # flag if LM was already applied to this beam\n",
    "        self.labeling = () # beam-labeling\n",
    "\n",
    "\n",
    "class BeamState:\n",
    "    \"information about the beams at specific time-step\"\n",
    "    def __init__(self):\n",
    "        self.entries = {}\n",
    "\n",
    "    def norm(self):\n",
    "        \"length-normalise LM score\"\n",
    "        for (k, _) in self.entries.items():\n",
    "            labelingLen = len(self.entries[k].labeling)\n",
    "            self.entries[k].prText = self.entries[k].prText ** (1.0 / (labelingLen if labelingLen else 1.0))\n",
    "\n",
    "    def sort(self):\n",
    "        \"return beam-labelings, sorted by probability\"\n",
    "        beams = [v for (_, v) in self.entries.items()]\n",
    "        sortedBeams = sorted(beams, reverse=True, key=lambda x: x.prTotal*x.prText)\n",
    "        return [x.labeling for x in sortedBeams]\n",
    "\n",
    "\n",
    "def applyLM(parentBeam, childBeam, classes, lm):\n",
    "    \"calculate LM score of child beam by taking score from parent beam and bigram probability of last two chars\"\n",
    "    if lm and not childBeam.lmApplied:\n",
    "        c1 = classes[parentBeam.labeling[-1] if parentBeam.labeling else classes.index(' ')] # first char\n",
    "        c2 = classes[childBeam.labeling[-1]] # second char\n",
    "        lmFactor = 0.01 # influence of language model\n",
    "        bigramProb = lm.getCharBigram(c1, c2) ** lmFactor # probability of seeing first and second char next to each other\n",
    "        childBeam.prText = parentBeam.prText * bigramProb # probability of char sequence\n",
    "        childBeam.lmApplied = True # only apply LM once per beam entry\n",
    "\n",
    "\n",
    "def addBeam(beamState, labeling):\n",
    "    \"add beam if it does not yet exist\"\n",
    "    if labeling not in beamState.entries:\n",
    "        beamState.entries[labeling] = BeamEntry()\n",
    "\n",
    "\n",
    "def ctcBeamSearch(mat, classes, lm, beamWidth=25):\n",
    "    \"beam search as described by the paper of Hwang et al. and the paper of Graves et al.\"\n",
    "\n",
    "    blankIdx = len(classes)\n",
    "    maxT, maxC = mat.shape\n",
    "\n",
    "    # initialise beam state\n",
    "    last = BeamState()\n",
    "    labeling = ()\n",
    "    last.entries[labeling] = BeamEntry()\n",
    "    last.entries[labeling].prBlank = 1\n",
    "    last.entries[labeling].prTotal = 1\n",
    "\n",
    "    # go over all time-steps\n",
    "    for t in range(maxT):\n",
    "        curr = BeamState()\n",
    "\n",
    "        # get beam-labelings of best beams\n",
    "        bestLabelings = last.sort()[0:beamWidth]\n",
    "\n",
    "        # go over best beams\n",
    "        for labeling in bestLabelings:\n",
    "\n",
    "            # probability of paths ending with a non-blank\n",
    "            prNonBlank = 0\n",
    "            # in case of non-empty beam\n",
    "            if labeling:\n",
    "                # probability of paths with repeated last char at the end\n",
    "                prNonBlank = last.entries[labeling].prNonBlank * mat[t, labeling[-1]]\n",
    "\n",
    "            # probability of paths ending with a blank\n",
    "            prBlank = (last.entries[labeling].prTotal) * mat[t, blankIdx]\n",
    "\n",
    "            # add beam at current time-step if needed\n",
    "            addBeam(curr, labeling)\n",
    "\n",
    "            # fill in data\n",
    "            curr.entries[labeling].labeling = labeling\n",
    "            curr.entries[labeling].prNonBlank += prNonBlank\n",
    "            curr.entries[labeling].prBlank += prBlank\n",
    "            curr.entries[labeling].prTotal += prBlank + prNonBlank\n",
    "            curr.entries[labeling].prText = last.entries[labeling].prText # beam-labeling not changed, therefore also LM score unchanged from\n",
    "            curr.entries[labeling].lmApplied = True # LM already applied at previous time-step for this beam-labeling\n",
    "\n",
    "            # extend current beam-labeling\n",
    "            for c in range(maxC - 1):\n",
    "                # add new char to current beam-labeling\n",
    "                newLabeling = labeling + (c,)\n",
    "\n",
    "                # if new labeling contains duplicate char at the end, only consider paths ending with a blank\n",
    "                if labeling and labeling[-1] == c:\n",
    "                    prNonBlank = mat[t, c] * last.entries[labeling].prBlank\n",
    "                else:\n",
    "                    prNonBlank = mat[t, c] * last.entries[labeling].prTotal\n",
    "\n",
    "                # add beam at current time-step if needed\n",
    "                addBeam(curr, newLabeling)\n",
    "                \n",
    "                # fill in data\n",
    "                curr.entries[newLabeling].labeling = newLabeling\n",
    "                curr.entries[newLabeling].prNonBlank += prNonBlank\n",
    "                curr.entries[newLabeling].prTotal += prNonBlank\n",
    "                \n",
    "                # apply LM\n",
    "                applyLM(curr.entries[labeling], curr.entries[newLabeling], classes, lm)\n",
    "\n",
    "        # set new beam state\n",
    "        last = curr\n",
    "\n",
    "    # normalise LM scores according to beam-labeling-length\n",
    "    last.norm()\n",
    "\n",
    "     # sort by probability\n",
    "    bestLabeling = last.sort()[0] # get most probable labeling\n",
    "\n",
    "    # map labels to chars\n",
    "    res = ''\n",
    "    for l in bestLabeling:\n",
    "        res += classes[l]\n",
    "\n",
    "    return res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
