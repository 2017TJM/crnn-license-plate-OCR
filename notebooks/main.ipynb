{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTMFkea5v1uo"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2U5ZoU3jv1up"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# warp_egg_path = '/opt/anaconda3/lib/python3.7/site-packages/warpctc_pytorch-0.1-py3.7-linux-x86_64.egg/warpctc_pytorch/__init__.py'\n",
    "# # warp_egg_path = '/usr/local/lib/python3.6/dist-packages/warpctc_pytorch-0.1-py3.6-linux-x86_64.egg'\n",
    "# sys.path.append(warp_egg_path)\n",
    "# from warpctc_pytorch import CTCLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AM3nRDI7v1ut"
   },
   "outputs": [],
   "source": [
    "# import self-defined jupyter notebook \n",
    "import nbimporter\n",
    "import utils\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZTOsIWsv1ux"
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import editdistance\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zTIaNck2v1uz"
   },
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LuWx_luv1u0"
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 30\n",
    "IMGH = 32\n",
    "IMGW = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTniM0Xav1u3"
   },
   "source": [
    "### Configure Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3hPTlQD9v1u3"
   },
   "outputs": [],
   "source": [
    "train_transformer = transforms.Compose([\n",
    "    transforms.Grayscale(),  \n",
    "    transforms.Resize((IMGH,IMGW)),\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(os.listdir('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3T38OlHv1u7"
   },
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEwX_4Ubv1vD",
    "outputId": "42c5a00b-72bd-46f8-8ee8-4d6a9aa1d293"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# train-validation split (80% train, 20% validation)\n",
    "from sklearn.model_selection import train_test_split\n",
    "n = range(len(os.listdir(PATH_TRAIN)))\n",
    "train_idx, val_idx = train_test_split(n, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oj4EJZLAv1vH"
   },
   "outputs": [],
   "source": [
    "# customise DataLoader\n",
    "class LPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A standard PyTorch definition of Dataset which defines the functions \n",
    "    __len__ and __getitem__.\n",
    "    \"\"\"\n",
    "    def __init__(self, path, cv_idx, transform):\n",
    "        \"\"\"\n",
    "        Store the filenames of the jpgs to use. \n",
    "        Specifies transforms to apply on images.\n",
    "\n",
    "        Args:\n",
    "            path: (string) directory containing the dataset\n",
    "            cv_idx: cross validation indices (training / validation sets)\n",
    "            transform: (torchvision.transforms) transformation to apply on image\n",
    "        \"\"\"\n",
    "        self.filenames = [os.listdir(path)[i] for i in cv_idx]\n",
    "        self.filenames = [os.path.join(path, f) for f in self.filenames \n",
    "                          if f.endswith('.jpg')]\n",
    "\n",
    "        self.labels = [filename.split('/')[-1].split('_')[-1].split('.')[0] \n",
    "                       for filename in self.filenames]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # return size of dataset\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch index idx image and labels from dataset. \n",
    "        Perform transforms on image.\n",
    "\n",
    "        Args:\n",
    "            idx: (int) index in [0, 1, ..., size_of_dataset-1]\n",
    "\n",
    "        Returns:\n",
    "            image: (Tensor) transformed image\n",
    "            label: corresponding label of image\n",
    "        \"\"\"\n",
    "        image = Image.open(self.filenames[idx])  # PIL image\n",
    "        image = self.transform(image)\n",
    "        return image, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o_ArjdSv1vK"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(LPDataset(PATH_TRAIN, train_idx, train_transformer), \n",
    "                          batch_size=BATCH_SIZE,  \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "648xsv93v1vM"
   },
   "outputs": [],
   "source": [
    "val_set = LPDataset(PATH_TRAIN, val_idx, train_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7j4WJX6v1vU"
   },
   "source": [
    "### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJDFEqn8v1vU",
    "outputId": "db0226be-257f-49b7-b7ea-2ca1af7b3adc"
   },
   "outputs": [],
   "source": [
    "# manualSeed = random.randint(1, 10000)  # fix seed\n",
    "# print(\"Random Seed: \", manualSeed)\n",
    "# random.seed(manualSeed)\n",
    "# np.random.seed(manualSeed)\n",
    "# torch.manual_seed(manualSeed)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WayUao1av1vY"
   },
   "outputs": [],
   "source": [
    "classes = string.ascii_uppercase+string.digits\n",
    "nclass = len(classes) + 1\n",
    "# number of channels 1=grayscale\n",
    "nc = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdY9rofIv1va"
   },
   "outputs": [],
   "source": [
    "converter = utils.strLabelConverter(classes)\n",
    "criterion = nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKE9LZBkv1vc"
   },
   "outputs": [],
   "source": [
    "# # custom weights initialization called on crnn\n",
    "# def weights_init(m):\n",
    "#     classname = m.__class__.__name__\n",
    "#     if classname.find('Conv') != -1:\n",
    "#         m.weight.data.normal_(0.0, 0.02)\n",
    "#     elif classname.find('BatchNorm') != -1:\n",
    "#         m.weight.data.normal_(1.0, 0.02)\n",
    "#         m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_PATH = '../model_weights/CRNN30.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRNN(imgH, nc, nclass, num_hidden(LSTM))\n",
    "crnn = model.CRNN(IMGH, nc, nclass, 256)\n",
    "crnn = torch.nn.DataParallel(crnn, range(1))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    crnn = crnn.cuda()\n",
    "    crnn.load_state_dict(torch.load(PRE_TRAINED_PATH))\n",
    "else:\n",
    "    crnn.load_state_dict(torch.load(PRE_TRAINED_PATH, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XrUds-Obv1vj"
   },
   "outputs": [],
   "source": [
    "image = torch.FloatTensor(BATCH_SIZE, 1, IMGH, IMGH)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    image.cuda()\n",
    "    \n",
    "text = torch.IntTensor(BATCH_SIZE * 5)\n",
    "length = torch.IntTensor(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate, Learning Rate Scheduler, Optimiser\n",
    "LR = 1e-1\n",
    "optimizer = optim.Adadelta(crnn.parameters(), lr=LR)\n",
    "T_max = len(train_loader) * EPOCH\n",
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=LR/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0E_TwA3v1vl"
   },
   "outputs": [],
   "source": [
    "loss_avg = utils.averager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ag3hpjNiv1vq"
   },
   "outputs": [],
   "source": [
    "def trainBatch(net, criterion, optimizer):\n",
    "    data = train_iter.next()\n",
    "    cpu_images, cpu_texts = data\n",
    "    batch_size = cpu_images.size(0)\n",
    "    utils.loadData(image, cpu_images)\n",
    "    t, l = converter.encode(cpu_texts)\n",
    "    utils.loadData(text, t)\n",
    "    utils.loadData(length, l)\n",
    "    \n",
    "    preds = crnn(image)\n",
    "    preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "    cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "    crnn.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwq18A1Ev1vt"
   },
   "outputs": [],
   "source": [
    "def validation(net, dataset, criterion, max_iter=100):\n",
    "    \"\"\"\n",
    "    To compute the validation loss from a given validation dataset\n",
    "    \n",
    "    net: neural network architecture\n",
    "    dataset: validation set\n",
    "    criterion: loss function\n",
    "    max_iter: maximum number of mini_batches\n",
    "    \n",
    "    return: validation loss\n",
    "    \"\"\"\n",
    "    \n",
    "    for p in crnn.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    # configure the mode: evaluation (model.train() & model.eval() behaves differently)\n",
    "    net.eval()\n",
    "    data_loader = DataLoader(\n",
    "        dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "    val_iter = iter(data_loader)\n",
    "\n",
    "    i = 0\n",
    "    loss_avg = utils.averager()\n",
    "    \n",
    "    max_iter = min(max_iter, len(data_loader))\n",
    "    for i in range(max_iter):\n",
    "        data = val_iter.next()\n",
    "        i += 1\n",
    "        cpu_images, cpu_texts = data\n",
    "        batch_size = cpu_images.size(0)\n",
    "        utils.loadData(image, cpu_images)\n",
    "        t, l = converter.encode(cpu_texts)\n",
    "        utils.loadData(text, t)\n",
    "        utils.loadData(length, l)\n",
    "\n",
    "        preds = crnn(image)\n",
    "        preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\n",
    "        cost = criterion(preds, text, preds_size, length) / batch_size\n",
    "        loss_avg.add(cost)\n",
    "\n",
    "    return loss_avg.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEnZhdz_v1vx",
    "outputId": "6b216cd0-3dbd-4333-9a4d-a5ed74a01fa5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/30][100/110] Train Loss: 0.001483  Validation Loss: 0.001088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|▎         | 1/30 [00:16<07:45, 16.07s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30][100/110] Train Loss: 0.001425  Validation Loss: 0.001105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  7%|▋         | 2/30 [00:32<07:29, 16.05s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/30][100/110] Train Loss: 0.001161  Validation Loss: 0.000908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 3/30 [00:48<07:12, 16.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/30][100/110] Train Loss: 0.001112  Validation Loss: 0.000897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 13%|█▎        | 4/30 [01:04<06:56, 16.02s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/30][100/110] Train Loss: 0.000974  Validation Loss: 0.000858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 17%|█▋        | 5/30 [01:20<06:40, 16.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5/30][100/110] Train Loss: 0.000899  Validation Loss: 0.000943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 20%|██        | 6/30 [01:36<06:24, 16.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6/30][100/110] Train Loss: 0.000948  Validation Loss: 0.001121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 23%|██▎       | 7/30 [01:52<06:08, 16.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7/30][100/110] Train Loss: 0.000888  Validation Loss: 0.000790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 27%|██▋       | 8/30 [02:08<05:52, 16.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8/30][100/110] Train Loss: 0.000745  Validation Loss: 0.000864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 30%|███       | 9/30 [02:24<05:36, 16.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9/30][100/110] Train Loss: 0.000732  Validation Loss: 0.000807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 33%|███▎      | 10/30 [02:40<05:20, 16.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/30][100/110] Train Loss: 0.000630  Validation Loss: 0.001004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 37%|███▋      | 11/30 [02:56<05:04, 16.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/30][100/110] Train Loss: 0.000598  Validation Loss: 0.000802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 40%|████      | 12/30 [03:12<04:48, 16.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/30][100/110] Train Loss: 0.000622  Validation Loss: 0.000986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 43%|████▎     | 13/30 [03:28<04:32, 16.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/30][100/110] Train Loss: 0.000591  Validation Loss: 0.000814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 47%|████▋     | 14/30 [03:44<04:16, 16.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/30][100/110] Train Loss: 0.000526  Validation Loss: 0.000866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 15/30 [04:00<04:00, 16.02s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/30][100/110] Train Loss: 0.000520  Validation Loss: 0.000820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 53%|█████▎    | 16/30 [04:16<03:44, 16.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/30][100/110] Train Loss: 0.000528  Validation Loss: 0.001130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 57%|█████▋    | 17/30 [04:32<03:28, 16.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/30][100/110] Train Loss: 0.000489  Validation Loss: 0.000958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60%|██████    | 18/30 [04:48<03:12, 16.03s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/30][100/110] Train Loss: 0.000470  Validation Loss: 0.000818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 63%|██████▎   | 19/30 [05:04<02:56, 16.02s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/30][100/110] Train Loss: 0.000440  Validation Loss: 0.000844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 67%|██████▋   | 20/30 [05:20<02:40, 16.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/30][100/110] Train Loss: 0.000406  Validation Loss: 0.000825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 70%|███████   | 21/30 [05:36<02:23, 16.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/30][100/110] Train Loss: 0.000423  Validation Loss: 0.000829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 73%|███████▎  | 22/30 [05:52<02:07, 15.99s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/30][100/110] Train Loss: 0.000406  Validation Loss: 0.000977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 77%|███████▋  | 23/30 [06:08<01:51, 15.99s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/30][100/110] Train Loss: 0.000394  Validation Loss: 0.000882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 80%|████████  | 24/30 [06:24<01:36, 16.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/30][100/110] Train Loss: 0.000393  Validation Loss: 0.000830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 83%|████████▎ | 25/30 [06:40<01:19, 16.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/30][100/110] Train Loss: 0.000439  Validation Loss: 0.000959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 87%|████████▋ | 26/30 [06:56<01:04, 16.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/30][100/110] Train Loss: 0.000378  Validation Loss: 0.001051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 90%|█████████ | 27/30 [07:12<00:48, 16.00s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/30][100/110] Train Loss: 0.000395  Validation Loss: 0.000904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 93%|█████████▎| 28/30 [07:28<00:32, 16.02s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/30][100/110] Train Loss: 0.000396  Validation Loss: 0.001098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 97%|█████████▋| 29/30 [07:44<00:16, 16.02s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/30][100/110] Train Loss: 0.000392  Validation Loss: 0.000862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 30/30 [08:00<00:00, 16.01s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# 25000 * 0.8 (# of data) // 64 (bs) ~= 310 (iterations) \n",
    "# NOTE: If no output => the training dataset is not fully loaded yet (i never reached 390)\n",
    "display_iter = len(os.listdir(PATH_TRAIN)) * 0.8 // BATCH_SIZE\n",
    "\n",
    "for epoch in trange(EPOCH):\n",
    "    train_iter = iter(train_loader)\n",
    "    i = 0\n",
    "\n",
    "    while i < len(train_loader):\n",
    "        for p in crnn.parameters():\n",
    "            p.requires_grad = True\n",
    "        crnn.train()\n",
    "        \n",
    "        cost = trainBatch(crnn, criterion, optimizer)\n",
    "        loss_avg.add(cost)\n",
    "        i += 1\n",
    "        if i % display_iter == 0:\n",
    "            # print training loss and validation loss\n",
    "            print('[%d/%d][%d/%d] Train Loss: %f  Validation Loss: %f' %\n",
    "                  (epoch, 30, i, len(train_loader), loss_avg.val(), validation(crnn, val_set, criterion)))\n",
    "            loss_avg.reset()\n",
    "            torch.save(crnn.state_dict(), 'experiments/netCRNN_{0}_{1}.pth'.format(epoch, i))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "main.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
